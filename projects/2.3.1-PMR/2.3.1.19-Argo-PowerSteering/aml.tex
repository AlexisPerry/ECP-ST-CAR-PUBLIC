\subsubsection*{AML}

\paragraph{Overview}

AML is a memory management library designed to ease the use of complex
memory topologies and complex data layout optimizations for
high-performance computing applications.

AML is a framework providing locality-preserving abstractions to
application developers.  In particular, AML aims to expose flexible
interfaces to describe and reason about how applications deal with data
layout, tiling of data, placement of data across hardware topologies, and
affinity between work and data.

\paragraph{Key Challenges}

Between non-uniform memory access (NUMA) to regular DRAM, the 3-D stacked
high-bandwidth memory, and the memory local to the accelerator devices such
as GPUs, the increasing depth of the memory hierarchy presents exascale
applications with a critical challenge of how to use the available
heterogeneous memory resources effectively.

Standardized interfaces to manage complex memory hierarchies are lacking,
and vendors are reluctant to innovate in this space in the absence of clear
directions from the community.  Coming up with an interface that is
sufficiently expressive to cover the emerging and projected hardware
advances, yet is simple enough and practical to be both acceptable and
useful to the applications is the key challenge that we are working on
addressing.

\paragraph{Solution Strategy}

AML provides explicit, application-aware memory management for deep memory
systems.  It offers a collection of building blocks that
are \emph{generic}, \emph{customizable}, and \emph{composable}.
Applications can specialize the implementation of each offered abstraction
and can mix and match the components as needed.  AML can be used to create,
for example, a software-managed scratchpad for multilevel DRAM hierarchy
such as HBM and DDR.  Such a scratchpad provides applications with a memory
region with a predictable high performance for critical data structures.

We provide applications and runtimes with a descriptive API for data
access, where all data placement decisions are explicit, and so is the
movement of data between different memory types.  At the same time, the API
does abstract the memory topology and other device complexities.  We focus
on optimizing data locality for current and future hardware generations;
applications provide insights for static allocations, and we can also
dynamically and asynchronously move or transform data to optimize for a particular 
device or to best take advantage of the memory hierarchy.

\begin{wrapfigure}[6]{r}{.18\textwidth}
%\vspace{-12pt}%
\includegraphics[width=.18\textwidth]{projects/2.3.1-PMR/2.3.1.19-Argo-PowerSteering/aml-components}
\end{wrapfigure}
AML components are built on top of hardware drivers and and system
libraries (libnuma, hwloc, cuda, opencl, SICM, umap).
The figure on the right depicts the major components of AML, including:
\begin{itemize}
\item Memory \emph{areas}, the location where data lives,
\item Applications data \emph{layouts} description,
\item \emph{DMA} engines, to move data across areas and layouts,
\item \emph{Tiling} schemes, the meta-structures on top of data layouts,
\item High level abstractions and helpers (\emph{scratchpad}, \emph{replicaset}).
\end{itemize}

This year AML contributions are distributed across new features,
infrastructure improvements, and new collaboration project.

\paragraph{Recent Progress}

%We developed AML, a memory library for explicit management of deep memory
%architectures.  Its main feature is a flexible and composable API, allowing
%applications to implement algorithms similar to out-of-core for deep
%memory.  We provided multiple optimized versions of memory migration
%facilities, ranging from a regular copy to a transparent move of memory
%pages, using synchronous and asynchronous interfaces and single- and
%multithreaded backends.  We validated the initial implementation on Intel's
%Knights Landing using a pipelining scheme for stencil applications.  We
%also identified interaction points between UMap and AML.  Further
%performance and capability improvements are underway.  In particular, we
%performed exhaustive studies comparing performance of various approaches
%for block-based DGEMM and task-based Cholesky decomposition.

%% AML development reached its first stable release recently.  We have a custom CI
%% pipeline in place that ensures this stability via automated testing (we are
%% making progress on leveraging the ECP CI infrastructure as well).  The
%% source code of version 0.1.0 is available on our website; we also provide
%% documentation on ReadTheDocs.  AML can also be installed via Spack.

%% We have been collaborating with the ExaSMR project on improving the
%% performance of the XSBench and RSBench proxy apps.  We applied the
%% topology-aware data replication facilities of AML in order to replicate
%% read-only and latency-sensitive data on low-latency memory nodes, improving
%% the code's behavior on NUMA architectures.  We found the effort to have a
%% limited impact on the application code, provided that the data is correctly
%% structured.  Performance was tested on four x86 architectures: Intel's
%% Knights Landing, Skylake, and Haswell, as well as AMD's Epyc.
%% Figure \ref{fig:argo:aml-results} outlines the results of these
%% experiments.

As part of our effort to improve applications locality, we merged
features on top of \emph{hwloc backend}. hwloc library exposes the machine
topology and its objects attributes. As a result, AML is able
to process the relationship between processing units and memories in
term of latency, bandwidth and hop distance. Additionally, AML is
able to make use of user provided distances. Hence, it becomes
possible to enhance available performance data with benchmarked metrics.
From AML user perspective a new \emph{area} base block implementation
is available. It implements the \emph{preferred} allocation policy on all
memories. These memories are ordered according to a performance
criterion (e.g bandwidth). Therefore, data can be allocated as long as
some memory is available. On top of that, it will be mapped on the best
available memories according the performance criterion. Furthermore,
we updated and merged the \emph{replicaset} high level abstraction which
is now available for users. In a nutshell, this abstraction will
create an area per NUMA cluster on the fastest memory of the cluster.
Then, the user can use the replicaset primitives to map data replicas,
and update the latter in these areas. We were using this trick to improve
\emph{XSBench} application (published this year). We forked the application
repository to include our latest changes in a branch and we are looking to
merge it soon in the main repository.

Finally, the last new capability we introduced in the library is the
use of \emph{excit} library as a submodule. Excit implements general
purpose iterators. We aim to use it to provide custom topology object
iterators. For instance, we want to be able to iterate memories of
a certain type or with specific performance abilities. In the future,
we also look to provide iterators for our \emph{tiling} abstraction.

As part of our infrastructure improvements, we bought a new machine
with heterogeneous hardware that we are using in our CI pipeline
to test new vendor backends. We also added a new CI stage:
Our development process implies that changes are branched off of the staging
branch. The latter is further tested and merged in the main master branch.
Our CI pipeline now includes a step where the merge of the staging branch into
the master branch will trigger a build and check on Theta machine.

%% \begin{figure}
%% \centering
%% \includegraphics[width=.8\textwidth]{projects/2.3.1-PMR/2.3.1.19-Argo-PowerSteering/aml-results}
%% \caption{Impact of the memory management policy used on the performance of
%% proxy apps.}
%% \label{fig:argo:aml-results}
%% \end{figure}

\paragraph{Next Steps}

%% We want to add support for more types of memory through the integration
%% with UMap and with other projects such as SICM.  We are planning to
%% significantly increase the topology-awareness of our interface through the
%% integration with Hwloc, by providing support for GPUs and other
%% accelerators and their topologies, and by extending the interface with
%% performance-oriented topology queries.  We are engaged in Aurora co-design
%% effort to ensure suitable hardware support on that platform.  Our
%% positive early results with ExaSMR proxy apps will be expanded to cover
%% the actual OpenMC application.

We are looking to build on top of our success with XSBench and use it as show
case of some of AML capabilities. Our next goal is to provide an \emph{OpenMP mapper}
like feature. OpenMP mapper allows to describe which part of a structure should be mapped onto
accelerator devices. It has recently been added to the standard. However, to the best
of our knowledge, no compiler implements it yet. This capability seems to fit in the
scope of our \emph{tiling} abstraction. It should work well with our \emph{DMA}
abstraction to manage its transfers back and forth onto devices. This block is also a
necessary step to be able to integrate with more complex applications such as OpenMC.

Finally, we are aiming at a broad compatibily of the library with accelerator devices.
Therefore, we are about to implement the base library abstractions on top of \emph{OpenCL}
backend.
